---
title: "QQM Mobility"
author: "Rowan Carew and Julia Gershenzon"
date: "2023-11-25"
output:
    html_document:
      toc: true
      toc_float: true
      number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

*The following document is divided into these sections:*
*1 - Setting Up. Use this for installing packages and uploading data*
*2 - Playground. Use this for testing code*
*3 - Descriptive Statistics*
*4 - Ordinal Models*
*5 - Choice Experiment Modelling*


# Setting Up

```{r}
#Load libraries
library(dplyr)
library(ggplot2)
library(naniar)
library(tidyverse)
library(janitor)
library(skimr)
library(psych)
library(ggdist)
library(car)
library(correlation)
library(scales)
```
```{r}
load("database_PT_final.Rda")
#NB - the data is called "database"

database_choices=read.csv("finaldata.csv",header=TRUE)

database<-inner_join(database_choices, database, by ='RID')
```

```{r}
#This code is for changing the variables to the correct data type.
database <-
  database %>%
  mutate(sex_front = as.factor(sex_front),
         p_bundesland_front = as.factor(p_bundesland_front),
         p_educ_front = as.factor(p_educ_front),
         age_front = as.integer(age_front),
         m_known = as.factor(m_known),
         DRT_knowledge = as.factor(DRT_knowledge),
         p_place_inhab = as.factor(p_place_inhab),
         p_mtools_1 = as.factor(p_mtools_1),
         p_central_wien = as.factor(p_central_wien),
         p_central_nonwien = as.factor(p_central_nonwien),
         kilometrage_per_car_1 = as.factor(kilometrage_per_car_1),
         kilometrage_per_car_2 = as.factor(kilometrage_per_car_2),
         kilometrage_per_car_3 = as.factor(kilometrage_per_car_3),
         kilometrage_per_car_4 = as.factor(kilometrage_per_car_4),
         n_o_v_hh = as.factor(n_o_v_hh),
         income = as.factor(income),
         children_1 = as.factor(children_1),
         children_2 = as.factor(children_2),
         children_3 = as.factor(children_3),
         status = as.factor(status),
         n_o_hhm = as.factor(n_o_hhm),
         p_season_1 = as.factor(p_season_1),
         p_season_2 = as.factor(p_season_2),
         p_season_3 = as.factor(p_season_3),
         p_season_4 = as.factor(p_season_4)
         )

glimpse(database)
```

```{r}
# Narrow down the dataset to only include rural inhabitants, which is <5,000 people.
database_rural <- subset(database, p_place_inhab %in% c(1,2))

database_nonrural <- subset(database, p_place_inhab %in% c(3,4,5))

```

# Playground
## Boomerism and filling out surveys

```{r}
table(database_rural$sex_front)

database_rural_boomer_filtered <- database_rural %>%
  filter(DURATION < quantile(DURATION, 0.95))

ggplot(data = database_rural_boomer_filtered) + 
  geom_point(mapping = aes(x = age_front, y = DURATION)) +
  geom_smooth(mapping = aes(x = age_front, y = DURATION)) +
ggtitle ("Boomerism and being shit at surveys")
```
```{r}
library(dplyr)
library(pspearman)
database_rural_boomer_filtered %>%
 dplyr::select(age_front, DURATION) %>%
  correlation(method = "spearman")
```

# Descriptive Statistics & Data Visualisations

## Rural Car Ownership across Austrian states

```{r}
#I want to create a clustered bar chart showing the different frequencies of whether people own a car or not in different Austrian states.

#First, I will rename the variables. This will make the graph more readable.
library(dplyr)

#Create a copy of the database to play with.
database_test <- database_rural

#Rename the variables. Start with value mapping (vm)
database_test__p_bundesland_vm <- c("1" = "Burgenland", "2" = "Kärnten", "3" = "Niederösterreich", "4" = "Oberösterreich", "5" = "Salzburg", "6" = "Steiermark", "7" = "Tirol", "8" = "Vorarlberg", "9" = "Wien", "10" = "Not Austria")

database_test_p_mtools_1_vm <- c("1" = "No", "2" = "Yes")

#Now I use the value mapping above to rename the values.
database_test <- database_test %>%
  mutate(p_bundesland_front = factor(p_bundesland_front, levels = as.character(1:10), labels = database_test__p_bundesland_vm))

database_test <- database_test %>%
  mutate(p_mtools_1 = factor(p_mtools_1, levels = as.character(1:2), labels = database_test_p_mtools_1_vm))



# Here I create a clustered bar chart.

library(ggplot2)
ggplot(database_test, aes(x = p_bundesland_front, fill = p_mtools_1)) +
  geom_bar(aes(y = after_stat(count/5)), position = "dodge") +
  labs(title = "Car ownership across Austrian states",
       x = "p_bundesland_front",
       y = "Frequency") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

```

## Mobility options in rural areas
```{r}
# Create a data frame
library(dplyr)
library(tidyr)
library(ggplot2)
library(reshape2)

#Create a copy of the database to play with.
database_test_p_mtools <- database_rural %>%
  dplyr::select(p_mtools_1, p_mtools_2, p_mtools_3, p_mtools_4, p_mtools_5, p_mtools_6)

#Show tables for the p_mtools variables. We will use the values from the tables to manually create a data frame with which we can create a clustered bar chart.Remember, the values need to be divided by 5, as we multiplied the data by 5 when merging the two datasets.

table(database_test_p_mtools$p_mtools_1)
table(database_test_p_mtools$p_mtools_2)
table(database_test_p_mtools$p_mtools_3)
table(database_test_p_mtools$p_mtools_4)
table(database_test_p_mtools$p_mtools_5)
table(database_test_p_mtools$p_mtools_6)

#Creating dataframes
database_test_p_mtools_df <- data.frame(
  "mobility.tool" = c("Own car", "Household Car", "Motorbike or Moped", "Bike or E-Bike", "Scooter or E-Scooter", "Carsharing Membership"),
  "no" = c(81, 291, 397, 235, 423, 445),
  "yes" = c(371, 161, 55, 217, 29, 7)
  )

#I now need to reshape the data, which will make it possible to plot in a clustered bar graph.
melted_data_p_mtools <- reshape2::melt(database_test_p_mtools_df, id.vars = "mobility.tool")

# Create a clustered bar chart
ggplot(melted_data_p_mtools, aes(x = `mobility.tool`, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Mobility options in rural areas",
       x = "mobility option",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

## Cars in different sized towns
*NB - VALUES HAVE YET TO BE DIVIDED BY 5*
```{r}
# Creating database that excludes NA data
database_inhab_complete <- subset(database, p_place_inhab %in% c(1,2,3,4,5))

# Vector assigning values 1 through 5 to the explanations of p_place_inhab used for labeling
inhabitants <- c("1" = "up to 1,000", "2" = "1,000 to 5,000", "3" = "5,000 to 50,000", "4" = "50,000 to 100,000", "5" = "more than 100.000")

# Converts p_place_inhab in database_inhab_complete into a factor variable with levels 1 to 5 and labels assigned from the inhabitants vector
database_inhab_complete <- database_inhab_complete %>%
    mutate(p_place_inhab = factor(p_place_inhab, 
                                          levels = as.character(1:5), 
                                          labels = inhabitants))
# Designs the clustered bar chart
ggplot(database_inhab_complete, aes(x = n_o_v_hh, fill = p_place_inhab)) +
  geom_bar(position = "dodge") +
  labs(title = "How many cars in which sized town?",
       x = "Number of cars",
       y = "Frequency") 
```

## Kilometres driven by owned cars
```{r}
# Create a data frame
library(dplyr)
library(tidyr)
library(ggplot2)
library(reshape2)
library(tidyverse)

#Create a copy of the database to play with.
database_test_km_driven <- database_rural %>%
  dplyr::select(kilometrage_per_car_1, kilometrage_per_car_2, kilometrage_per_car_3, kilometrage_per_car_4)

#Show tables for the kilometrage_per_car variables. We will use the values from the tables to manually create a data frame with which we can create a clustered bar chart.Remember, the values need to be divided by 5, as we multiplied the data by 5 when merging the two datasets.

table(database_test_km_driven$kilometrage_per_car_1)
table(database_test_km_driven$kilometrage_per_car_2)
table(database_test_km_driven$kilometrage_per_car_3)
table(database_test_km_driven$kilometrage_per_car_4)

#Creating dataframes. Use values from the table divided by 5.
database_test_km_driven_df <- data.frame(
  "km.driven" = c("car 1", "car 2", "car 3", "car 4"),
  "1_or_less" = c(12, 11, 63, 36),
  "1_to_5" = c(31, 35, 54, 9),
  "5_to_10" = c(45, 64, 56, 4),
  "10_to_15" =c(44, 56, 31, 5),
  "1_to_25" =c(22, 44, 19, 3),
  "25_or_more" =c(18, 27, 14, 4)
  )

#I now need to reshape the data, which will make it possible to plot in a clustered bar graph.
melted_data_km_driven <- reshape2::melt(database_test_km_driven_df, id.vars = "km.driven")
melted_data_km_driven <- rename(melted_data_km_driven, km.driven.thousands = variable)

# Create a clustered bar chart
ggplot(melted_data_km_driven, aes(x = `km.driven`, y = value, fill = km.driven.thousands)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Kilometres driven by vehicle",
       x = "Which car",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

# Correlation Analysis
## Rural Income and No. Vehicles and Season Ticket
```{r}
database_rural <-
  database_rural %>%
  mutate(sex_front = as.factor(sex_front),
         p_bundesland_front = as.factor(p_bundesland_front),
         p_educ_front = as.factor(p_educ_front),
         age_front = as.integer(age_front),
         m_known = as.factor(m_known),
         DRT_knowledge = as.factor(DRT_knowledge),
         p_place_inhab = as.factor(p_place_inhab),
         p_mtools_1 = as.factor(p_mtools_1),
         p_central_wien = as.factor(p_central_wien),
         p_central_nonwien = as.factor(p_central_nonwien),
         kilometrage_per_car_1 = as.factor(kilometrage_per_car_1),
         kilometrage_per_car_2 = as.factor(kilometrage_per_car_2),
         kilometrage_per_car_3 = as.factor(kilometrage_per_car_3),
         kilometrage_per_car_4 = as.factor(kilometrage_per_car_4),
         n_o_v_hh = as.factor(n_o_v_hh),
         income = as.factor(income),
         children_1 = as.factor(children_1),
         children_2 = as.factor(children_2),
         children_3 = as.factor(children_3),
         status = as.factor(status),
         n_o_hhm = as.factor(n_o_hhm),
         p_season_1 = as.factor(p_season_1),
         p_season_2 = as.factor(p_season_2),
         p_season_3 = as.factor(p_season_3),
         p_season_4 = as.factor(p_season_4)
         )

library(polycor)
poly_corr_income <- hetcor(database_rural[, c('income', 'p_season_1', 'p_season_2', 'p_season_3', 'p_season_4', 'n_o_v_hh')], ML = TRUE)
print(poly_corr_income)

#Interpreting these results:
#Table 1 shoes correlations. A value of "0" means no correlation. -1 implies a perfect negative correlation, and 1 implies a positive correlation. The main finding here is that income has a weak positive correlation with number of household vehicles.
#Table 2 shows standard deviations.
#Table 3 shows p-values. Any value <0.05 is significant.

#Overall, the only variable that income has no statistically significant relationship with is p_season_4, which is ownership of an OBB Vorteilscard.
```
### Regression Test
```{r}
library(MASS)
library(lmtest)
library(AER)
library(modelsummary)
rural_income_model <- polr(income ~ p_season_1 + p_season_2 + p_season_3 + p_season_4 + n_o_v_hh, data = database_rural, Hess=TRUE)
summary(rural_income_model)

# QUESTION FOR STEFANIE: The regression model above gives us coefficients, std. errors and t values. The t values represent measures of how many standard errors the estimated coefficient is away from zero. A larger t-value indicates a stronger evidence against the null hypothesis that the true coefficient is zero.

#Now to calculate the p-values:
tidy_custom.polr <- function(x, ...) {
  s <- coeftest(x)
  out <- data.frame(
    term = row.names(s),
    p.value = s[, "Pr(>|z|)"])
  out
}

mod = list(
  "LM" = lm(income ~ p_season_1 + p_season_2 + p_season_3 + p_season_4 + n_o_v_hh, data = database_rural),
  "POLR" = polr(as.ordered(income) ~ p_season_1 + p_season_2 + p_season_3 + p_season_4 + n_o_v_hh, data = database_rural))

modelsummary(mod, stars = TRUE)

#https://modelsummary.com/articles/modelsummary.html#adding-new-information-to-existing-models-1
```

# Choice Models

## Set Up
```{r}
#First, we need to rename some variables. Commented this out because once the dataset has been renamed once, this code no longer works.
database_rural <- database_rural %>% rename("Spatial_1" = "a1_x2",
                          "Temporal_1" = "a1_x3",
                          "DRTPattern_1" = "a1_x4",
                          "Waitingtime_1" = "a1_x5",
                          "Discount_1" = "a1_x6",
                          "Compensation_1" = "a1_x7",
                          "Price_1" = "a1_x8",
                          "Spatial_2" = "a3_x2",
                          "Temporal_2" = "a2_x3",
                          "DRTPattern_2" = "a2_x4",
                          "Waitingtime_2" = "a2_x5",
                          "Discount_2" = "a2_x6",
                          "Compensation_2" = "a2_x7",
                          "Price_2" = "a2_x8",
                          "CHOICE" = "pref1"
                          )

#Now to make sure they are correctly shown to be ordinal variables.
database_rural <-
  database_rural %>%
  mutate(Spatial_1 = as.factor(Spatial_1),
         Temporal_1 = as.factor(Temporal_1),
         DRTPattern_1 = as.factor(DRTPattern_1),
         Waitingtime_1 = as.factor(Waitingtime_1),
         Discount_1 = as.factor(Discount_1),
         Compensation_1 = as.factor(Compensation_1),
         Price_1 = as.factor(Price_1),
         Spatial_2 = as.factor(Spatial_2),
         Temporal_2 = as.factor(Temporal_2),
         DRTPattern_2 = as.factor(DRTPattern_2),
         Waitingtime_2 = as.factor(Waitingtime_2),
         Discount_2 = as.factor(Discount_2),
         Compensation_2 = as.factor(Compensation_2),
         Price_2 = as.factor(Price_2),
         CHOICE = as.factor(CHOICE)
         )
```

## Apollo
```{r}
# Load apollo library and core settings                                                                                

### Clear memory
#rm(list = ls())

### Load Apollo library
#library(apollo)

### Initialise code
#apollo_initialise()

### Set core controls
#apollo_control = list(
  #modelName  ="Apollo_example_1",
  #modelDescr ="TESTING GIVE BETTER DESCRIPTION LATER",
  #indivID    ="ID",
  #outputDirectory = "output"
#)

# ################################################################# #
#### DEFINE MODEL PARAMETERS                                     ####
# ################################################################# #

### Vector of parameters, including any that are kept fixed in estimation
#apollo_beta=c(
 #             b_spatial  = 0,
 #             b_temporal  = 0,
#              b_drtpattern  = 0,
  #            b_waiting = 0,
  #            b_discount  = 0,
  ##            b_compensation  = 0,
  #            b_price = 0
  #        )

### TAKEN FROM STEFANIE CODE. Vector with names (in quotes) of parameters to be kept fixed at their starting value in apollo_beta, use apollo_beta_fixed = c() if none

#apollo_fixed = c()

# ################################################################# #
#### GROUP AND VALIDATE INPUTS                                   ####
# ################################################################# #

#apollo_inputs = apollo_validateInputs()

#OK so this is throwing up an error probably for one of these reasons: "This function takes no arguments but looks in the global environment for the various inputs required for a model. This always includes the control settings apollo_control, the model parameters apollo_beta, the vector with names of fixed parameters apollo_fixed and finally the data object database. If any of these objects are missing from the global environment, the execution of apollo_validateInputs fails." 

# ################################################################# #
#### DEFINE MODEL AND LIKELIHOOD FUNCTION                        ####
# ################################################################# #

#=function(apollo_beta, apollo_inputs, functionality="estimate"){
  
  ### Attach inputs and detach after function exit
#  apollo_attach(apollo_beta, apollo_inputs)
#  on.exit(apollo_detach(apollo_beta, apollo_inputs))
  
  ### Create list of probabilities P
#  P = list()
  
  ### List of utilities: these must use the same names as in mnl_settings, order is irrelevant
#  V = list()
  
#   V[["alt1"]]  = b_spatial  * Spatial_1 + b_temporal * Temporal_1 + b_drtpattern * DRTPattern_1 +
#     b_waiting * Waitingtime_1 + b_discount * Discount_1 + b_compensation * Compensation_1 + b_price * Price_1
#   V[["alt2"]]  = b_spatial  * Spatial_2 + b_temporal * Temporal_2 + b_drtpattern * DRTPattern_2 +
 #    b_waiting * Waitingtime_2 + b_discount * Discount_2 + b_compensation * Compensation_2 + b_price * Price_2
   
     ### Define settings for MNL model component
#  mnl_settings = list(
#    alternatives  = c(alt1=1, alt2=2), 
#    avail         = 1, 
#    choiceVar     = CHOICE,
#    utilities     = V
#  )
  
  ### Compute probabilities using MNL model
#  P[["model"]] = apollo_mnl(mnl_settings, functionality)
  
  ### Take product across observation for same individual
#  P = apollo_panelProd(P, apollo_inputs, functionality)
  
  ### Prepare and return outputs of function
#  P = apollo_prepareProb(P, apollo_inputs, functionality)
#  return(P)
#    }

# ################################################################# #
#### MODEL ESTIMATION                                            ####
# ################################################################# #

#model = apollo_estimate(apollo_beta, apollo_fixed, apollo_probabilities, apollo_inputs)

# ################################################################# #
#### MODEL OUTPUTS                                               ####
# ################################################################# #

# ----------------------------------------------------------------- #
#---- FORMATTED OUTPUT (TO SCREEN)                               ----
# ----------------------------------------------------------------- #

#apollo_modelOutput(model)

# ----------------------------------------------------------------- #
#---- FORMATTED OUTPUT (TO FILE, using model name)               ----
# ----------------------------------------------------------------- #

#apollo_saveOutput(model)



# deltaMethod_settings=list(expression=c(WTP_spatial="b_spatial/b_price",
         ##                              WTP_temporal  = "b_temporal/b_price",
          #                             WTP_drtpattern  = "b_drtpattern/b_price",
          ##                             WTP_waiting = "b_waiting/b_price",
          #                             WTP_discount  = "b_discount/b_price",
           #                            WTP_compensation  = "b_compensation/b_price"))
 
#apollo_deltaMethod(model, deltaMethod_settings)
```


## Multinomial Logit

## Nested Multinomial Logit

## Latent Class Model

